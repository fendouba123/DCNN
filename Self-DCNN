import torch
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import StratifiedKFold
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, recall_score, matthews_corrcoef, roc_curve, confusion_matrix
import numpy as np
import pandas as pd


class SelfAttentionModule(nn.Module):
    def __init__(self, embed_size, heads=8):
        super(SelfAttentionModule, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        assert self.head_dim * heads == embed_size, "Embed size needs to be divisible by heads"
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, x):
        batch_size, seq_len, embed_size = x.shape
        heads = self.heads
        x = x.view(batch_size, seq_len, heads, self.head_dim).transpose(1, 2)
        queries, keys, values = [l(x).view(batch_size, heads, seq_len, self.head_dim).transpose(1, 2)
                                 for l, x in zip([self.queries, self.keys, self.values], (x, x, x))]
        scores = (queries @ keys.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        out = attention_weights @ values
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        out = self.fc_out(out)

        return out

class CNNModel(nn.Module):
    def __init__(self, input_shape, num_classes=1, dropout_rate=0.2, out_channels=4,embed_size=8,l2_regularization=0.01):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=2, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.fc1 = nn.Linear(out_channels * (input_shape[0] // 2 + 1), embed_size)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.self_attention = SelfAttentionModule(embed_size, heads=2)
        self.fc2 = nn.Linear(embed_size, num_classes)
        self.l2_regularization = l2_regularization

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        context = self.self_attention(x.unsqueeze(1))
        x = F.relu(self.fc1(context))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def read_data(file, sep=',', label_col_index=-1):
    df = pd.read_csv(file, sep=sep)
    if df.shape[1] < 2:
        raise ValueError("Data must contain at least one feature column and one label column.")
    X = df.iloc[:, :-1].values.astype('float32')
    y = df.iloc[:, label_col_index].values
    if X.shape[1] == 0:
        raise ValueError("Feature set is empty.")
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    return X, y_encoded, label_encoder



def train_model(model, train_loader, optimizer, criterion, device, epochs):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            if labels.dim() == 1:
                labels = labels.view(-1, 1)
            optimizer.zero_grad()
            outputs = model(inputs)
            if outputs.dim() != 2 or outputs.size(1) != 1:
                outputs = outputs.view(-1, 1)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
        print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}')
    return model, epoch_loss

def evaluate_model(model, test_loader, device):
    model.eval()
    y_pred_probs = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            logits = model(inputs)
            probs = torch.sigmoid(logits).cpu().numpy()
            y_pred_probs.extend((probs > 0.5).astype(np.int32).flatten())
            y_true.extend(labels.cpu().numpy().flatten())
    y_pred = np.array(y_pred_probs)
    y_true = np.array(y_true)
    return y_pred, y_true

def Cross_test(X, y, n_fold=10, batch_size=4, epochs=5, learning_rate=0.0001, out_channels=4,l2_regularization=0.01):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    input_shape = (X.shape[1], 1)
    num_classes = 1
    model = CNNModel(input_shape=input_shape, num_classes=num_classes, out_channels=out_channels)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)
    model = model.to(device)

    skf = StratifiedKFold(n_splits=n_fold)
    fold_results = {
        'AUC': [], 'Recall': [],'Sensitivity': [], 'Specificity': [], 'Accuracy': [],
        'MCC': [], 'Losses': []
    }

    columns = ['AUC', 'Recall', 'Sensitivity', 'Specificity', 'Accuracy', 'MCC', 'Loss']
    df_results = pd.DataFrame(index=range(1, n_fold + 1), columns=columns)

    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
        print(f'Training fold {fold + 1}')
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        X_train = np.expand_dims(X_train, axis=1)
        X_test = np.expand_dims(X_test, axis=1)
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)

        trained_model, epoch_loss = train_model(model, train_loader, optimizer, criterion, device, epochs)
        fold_results['Losses'].append(epoch_loss)


        torch.save(trained_model.state_dict(), f'model_fold_{fold + 1}.pt')
        y_pred_probs, y_true = evaluate_model(trained_model, test_loader, device)
        y_pred = (y_pred_probs > 0.5).astype(int)  # 确保y_pred只包含0和1
        if not np.all((y_true == 0) | (y_true == 1)):
            raise ValueError("y_true contains values other than 0 or 1")

        con_mat = confusion_matrix(y_true, y_pred)
        auc = roc_auc_score(y_true, y_pred_probs)
        recall = recall_score(y_true, y_pred)
        sensitivity=con_mat[1, 1] / (con_mat[1, 1] + con_mat[1, 0])
        specificity = con_mat[0, 0] / (con_mat[0, 0] + con_mat[0, 1])
        accuracy = (con_mat[1, 1] + con_mat[0, 0]) / (con_mat[1, 1] + con_mat[1, 0] + con_mat[0, 0] + con_mat[0, 1])
        mcc = matthews_corrcoef(y_true, y_pred)

        fold_results['AUC'].append(auc)
        fold_results['Recall'].append(recall)
        fold_results['Sensitivity'].append(sensitivity)
        fold_results['Specificity'].append(specificity)
        fold_results['Accuracy'].append(accuracy)
        fold_results['MCC'].append(mcc)
        fold_results['Losses'].append(epoch_loss)
        df_results.loc[fold + 1] = [auc, recall, sensitivity, specificity, accuracy, mcc, epoch_loss]

        print(f'Fold {fold + 1} Results:')
        print(f'AUC: {auc:.4f}')
        print(f'Recall: {recall:.4f}')
        print(f'Sensitivity: {sensitivity:.4f}')
        print(f'Specificity: {specificity:.4f}')
        print(f'Accuracy: {accuracy:.4f}')
        print(f'MCC: {mcc:.4f}')
        print(f'Loss: {epoch_loss:.4f}')

    df_results['Average'] = df_results.mean(axis=0)
    df_results.index = range(1, n_fold + 1)
    df_results.to_csv('cross_results.csv', index_label='Fold')
    return fold_results, df_results

if __name__ == '__main__':
    file_path = 'file.csv'
    try:
        X, y, label_encoder = read_data(file_path, label_col_index=-1)
    except ValueError as e:
        print(e)
        exit()
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")

    smote = SMOTE(sampling_strategy='not majority')
    X_sm, y_sm = smote.fit_resample(X, y)

    results, df_results = Cross_test(X_sm, y_sm, out_channels=4)

    for key, value in  results.items():
        if isinstance(value, list):
            print(f'Average {key}: {np.mean(value):.4f}')
        else:
            print(f'Average {key}: {value}')
