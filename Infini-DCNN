import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import StratifiedKFold
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, recall_score, matthews_corrcoef, roc_curve, confusion_matrix
import numpy as np
import pandas as pd


class InfiniteAttentionModule(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(InfiniteAttentionModule, self).__init__()
        self.hidden_dim = hidden_dim
        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)
        self.W = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        batch_size, seq_length, _ = x.shape
        h0 = torch.zeros(1, batch_size, self.hidden_dim).to(x.device)
        outputs, _ = self.rnn(x, h0)
        outputs = outputs.contiguous().view(-1, self.hidden_dim)
        attention_scores = F.softmax(self.W(outputs), dim=1)
        attention_applied = x * attention_scores.unsqueeze(-1)
        aggregated_features = torch.sum(attention_applied, dim=1)
        return aggregated_features, attention_scores


class CNNModel(nn.Module):
    def __init__(self, input_shape, dropout_rate=0.4, out_channels=4,out_channels_1=8, l2_regularization=0.01):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=2, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.attention = InfiniteAttentionModule(out_channels_1, out_channels_1)
        self.fc1 = nn.Linear(out_channels * (input_shape[0] // 2 + 1), out_channels_1)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.fc2 = nn.Linear(out_channels_1, 1)
        self.l2_regularization = l2_regularization

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = x.view(context.size(0), -1)
        context, _ = self.attention(x.unsqueeze(1))
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x).squeeze()
        return x


def read_data(file, sep=',', label_col_index=-1):
    df = pd.read_csv(file, sep=sep)
    if df.shape[1] < 2:
        raise ValueError("Data must contain at least one feature column and one label column.")
    X = df.iloc[:, :-1].values.astype('float32')
    y = df.iloc[:, label_col_index].values
    if X.shape[1] == 0:
        raise ValueError("Feature set is empty.")
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    return X, y_encoded, label_encoder


def train_model(model, train_loader, optimizer, criterion, device, epochs):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            labels = labels.view(-1, 1)
            optimizer.zero_grad()
            outputs = model(inputs)
            outputs = outputs.view(-1, 1)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
    return model, epoch_loss


def evaluate_model(model, test_loader, device):
    model.eval()
    y_pred_probs = []
    y_true = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            logits = model(inputs)
            probs = torch.sigmoid(logits).squeeze().cpu().numpy()
            y_pred_probs.extend(probs.ravel())
            y_true.extend(labels.cpu().numpy())
    return np.array(y_pred_probs), np.array(y_true)


def Cross_test(X, y, n_fold=10, batch_size=16, epochs=5, learning_rate=0.001, out_channels=4, l2_regularization=0.01):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    input_shape = (X.shape[1], 1)
    num_classes = 1
    model = CNNModel(input_shape=input_shape, num_classes=num_classes, out_channels=out_channels)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)
    model = model.to(device)

    skf = StratifiedKFold(n_splits=n_fold)
    fold_results = {
        'AUC': [], 'Recall': [], 'Sensitivity': [], 'Specificity': [], 'Accuracy': [],
        'MCC': [], 'Losses': []
    }

    columns = ['AUC', 'Recall', 'Sensitivity', 'Specificity', 'Accuracy', 'MCC', 'Loss']
    df_results = pd.DataFrame(index=range(1, n_fold + 1), columns=columns)

    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
        print(f'Training fold {fold + 1}')
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        X_train = np.expand_dims(X_train, axis=1)
        X_test = np.expand_dims(X_test, axis=1)
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)

        trained_model, epoch_loss = train_model(model, train_loader, optimizer, criterion, device, epochs)
        fold_results['Losses'].append(epoch_loss)
        torch.save(trained_model.state_dict(), f'model_fold_{fold + 1}.pt')

        y_pred_probs, y_true = evaluate_model(trained_model, test_loader, device)
        y_pred = (y_pred_probs > 0.5).astype(int)

        con_mat = confusion_matrix(y_true, y_pred)
        auc = roc_auc_score(y_true, y_pred_probs)
        recall = recall_score(y_true, y_pred)
        sensitivity = con_mat[1, 1] / (con_mat[1, 1] + con_mat[1, 0])
        specificity = con_mat[0, 0] / (con_mat[0, 0] + con_mat[0, 1])
        accuracy = (con_mat[1, 1] + con_mat[0, 0]) / (con_mat[1, 1] + con_mat[1, 0] + con_mat[0, 0] + con_mat[0, 1])
        mcc = matthews_corrcoef(y_true, y_pred)

        fold_results['AUC'].append(auc)
        fold_results['Recall'].append(recall)
        fold_results['Sensitivity'].append(sensitivity)
        fold_results['Specificity'].append(specificity)
        fold_results['Accuracy'].append(accuracy)
        fold_results['MCC'].append(mcc)
        fold_results['Losses'].append(epoch_loss)

        df_results.loc[fold + 1] = [auc, recall, sensitivity, specificity, accuracy, mcc, epoch_loss]

        print(f'Fold {fold + 1} Results:')
        print(f'AUC: {auc:.4f}')
        print(f'Recall: {recall:.4f}')
        print(f'Sensitivity: {sensitivity:.4f}')
        print(f'Specificity: {specificity:.4f}')
        print(f'Accuracy: {accuracy:.4f}')
        print(f'MCC: {mcc:.4f}')
        print(f'Loss: {epoch_loss:.4f}')

    df_results['Average'] = df_results.mean(axis=0)
    df_results.index = range(1, n_fold + 1)
    df_results.to_csv('cross_results.csv', index_label='Fold')
    return fold_results, df_results


if __name__ == '__main__':
    file_path = 'ca.csv'
    try:
        X, y, label_encoder = read_data(file_path, label_col_index=-1)
    except ValueError as e:
        print(e)
        exit()
    print(f"X shape: {X.shape}")
    print(f"y shape: {y.shape}")

    smote = SMOTE(sampling_strategy='not majority')

    X_sm, y_sm = smote.fit_resample(X, y)

    results, df_results = Cross_test(X_sm, y_sm, out_channels=8)

    for key, value in results.items():
        if isinstance(value, list):
            print(f'Average {key}: {np.mean(value):.4f}')
        else:
            print(f'Average {key}: {value}')

